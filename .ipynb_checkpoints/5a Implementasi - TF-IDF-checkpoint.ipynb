{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#import numpy as np\n",
    "#from sklearn import svm\n",
    "#from sklearn.metrics import accuracy_score, precision_score\n",
    "#from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fungsi cleaning\n",
    "def cleaning2(text):\n",
    "    text = re.sub(r'\\b(?:(?:https?|ftp)://)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*(?<![.,])', ' ', text.lower())\n",
    "    #text = re.sub(r'\\b(?:(?:https?|ftp)://)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*(?<!.,)', ' ', text.lower())\n",
    "    words = re.findall(r'[a-z0-9]+', text)\n",
    "    return ' '.join(words)\n",
    "\n",
    "    #fungsi hapus\n",
    "def hapus(n_komentar):\n",
    "    clean_komentar = []\n",
    "    clean_komentar_hapus = []\n",
    "    for i in range(len(n_komentar)):\n",
    "        x = str(n_komentar[i])\n",
    "        splt = x.split(\" \")\n",
    "        for j in range(len(splt)):\n",
    "            for k in range(len(diganti)):\n",
    "                if splt[j] == diganti[k]:\n",
    "                    splt[j]= ganti[k]\n",
    "        join = ' '.join(map(str,(splt)))\n",
    "        clean_komentar.append(join)\n",
    "    \n",
    "    for i in range(len(clean_komentar)):\n",
    "        x = str(clean_komentar[i])\n",
    "        splt = x.split(\" \")\n",
    "        for j in range(len(splt)):\n",
    "            for k in range(len(hapus)):\n",
    "                if splt[j] == hapus[k]:\n",
    "                    splt[j]= ''\n",
    "        join = ' '.join(map(str,(splt)))\n",
    "        clean_komentar_hapus.append(join)\n",
    "\n",
    "def normalisasi(n_komentar):\n",
    "    clean_komentar = []\n",
    "    clean_komentar_hapus = []\n",
    "    for i in range(len(n_komentar)):\n",
    "        x = str(n_komentar[i]) \n",
    "        splt = x.split(\" \")\n",
    "        for j in range(len(splt)):\n",
    "            for k in range(len(diganti)):\n",
    "                if splt[j] == diganti[k]:\n",
    "                    splt[j]= ganti[k]\n",
    "        join = ' '.join(map(str,(splt)))\n",
    "        clean_komentar.append(join)\n",
    "    \n",
    "    for i in range(len(clean_komentar)):\n",
    "        x = str(clean_komentar[i])\n",
    "        splt = x.split(\" \")\n",
    "        for j in range(len(splt)):\n",
    "            for k in range(len(hapuss)):\n",
    "                if splt[j] == hapuss[k]:\n",
    "                    splt[j]= ''\n",
    "        join = ' '.join(map(str,(splt)))\n",
    "        clean_komentar_hapus.append(join)\n",
    "    return str(clean_komentar_hapus)\n",
    "    \n",
    "# sastrawi untuk proses stemming\n",
    "factory = StemmerFactory()\n",
    "factoryStop = StopWordRemoverFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stopword = factoryStop.create_stop_word_remover()\n",
    "\n",
    "#load dataset untk normalisasi\n",
    "data_replace  = pd.read_excel('Corpus_kata_replace_new.xlsx', sheet_name = 'kata_repalce')\n",
    "data_hapus  = pd.read_excel('Corpus_kata_replace_new.xlsx', sheet_name = 'kata_hapus')\n",
    "diganti = data_replace['kata'].tolist()\n",
    "ganti = data_replace['ganti'].tolist()\n",
    "hapuss = data_hapus['kata'].tolist()\n",
    "\n",
    "\n",
    "# load the features from disk\n",
    "x = open('model/feature.txt')\n",
    "x = x.read()\n",
    "koso_kata = x.split()\n",
    "tfidf = TfidfVectorizer(vocabulary=koso_kata)\n",
    "\n",
    "# load the model from disk\n",
    "comNB = pickle.load(open('model\\comNB.sav', 'rb'))\n",
    "cSVM = pickle.load(open('model\\cSVM.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komentar: sadas\n",
      "==============================================================================================================================\n",
      "['sadas']\n",
      "==============================================================================================================================\n",
      "Prediksi SVM (TF-IDF)           :  ['bukan spam']\n",
      "Prediksi C Naive Bayes (TF-IDF) :  ['bukan spam']\n",
      "(1, 3010)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#input_komentar = \"Cek IG kami Kak, solusiii naiiikkk tinggiiii sampe 175cm, bahkan sampe umuur 33th masih bisa looh, Buruaaaaannn. '\"\n",
    "input_komentar = input(\"Komentar: \")\n",
    "komentar  = [cleaning2(input_komentar)]\n",
    "komentar0 = normalisasi(komentar)\n",
    "komentar = stemmer.stem(str(komentar0))\n",
    "komentar = stopword.remove(komentar)\n",
    "print(\"==============================================================================================================================\")\n",
    "print(komentar0)\n",
    "print(\"==============================================================================================================================\")\n",
    "\n",
    "#t_clean_komentar_hapus = komentar_akhir\n",
    "t_clean_komentar_hapus = []\n",
    "t_clean_komentar_hapus.append(komentar)\n",
    "#iki = loaded_vec.fit_transform(np.array(t_clean_komentar_hapus))\n",
    "t_tfidf_matrix = tfidf.fit_transform(t_clean_komentar_hapus)\n",
    "\n",
    "data_X = t_tfidf_matrix.toarray()\n",
    "#cek \n",
    "#x_cek = data_X[len(t_clean_komentar_hapus)-1:len(t_clean_komentar_hapus)]\n",
    "x_cek = data_X[len(t_clean_komentar_hapus)-1:len(t_clean_komentar_hapus)]\n",
    "\n",
    "# prediksi data asli\n",
    "Y_SVM = cSVM.predict(x_cek)\n",
    "Y_NB = comNB.predict(x_cek)\n",
    "\n",
    "# print prediksi\n",
    "print(\"Prediksi SVM (TF-IDF)           : \", Y_SVM)\n",
    "print(\"Prediksi C Naive Bayes (TF-IDF) : \", Y_NB)\n",
    "print(t_tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3010"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
